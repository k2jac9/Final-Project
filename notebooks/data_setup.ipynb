{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Any, List\n",
    "from functools import wraps\n",
    "import colorlog\n",
    "import time\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import backoff\n",
    "from joblib import Parallel, delayed\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mData setup initiated...\u001b[0m\n",
      "\u001b[37mdownload_and_extract_kaggle_datasets executed in 0.56 seconds.\u001b[0m\n",
      "\u001b[37mFound 7 total CSV files in the datasets.\u001b[0m\n",
      "\u001b[37mfind_csv_files executed in 0.08 seconds.\u001b[0m\n",
      "\u001b[37mDataset extracted to ../data/food-com-recipes-and-user-interactions\n",
      "CSV files: interactions_test.csv, interactions_train.csv, interactions_validation.csv, PP_recipes.csv, PP_users.csv, RAW_interactions.csv, RAW_recipes.csv\u001b[0m\n",
      "\u001b[32mDataset setup completed.\u001b[0m\n",
      "\u001b[37mmain executed in 0.64 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO').upper()\n",
    "    DATA_DIRECTORY = Path(os.getenv('DATA_DIRECTORY', '../data'))\n",
    "    RETRY_LIMIT = int(os.getenv('RETRY_LIMIT', 2))\n",
    "    N_JOBS = int(os.getenv('N_JOBS', -1))  # Utilize all available CPUs if set to -1\n",
    "    LOG_FILE = os.getenv('LOG_FILE', 'app.log')\n",
    "    LOG_FORMAT_CONSOLE = '%(log_color)s%(message)s%(reset)s'\n",
    "    LOG_FORMAT_FILE = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "    DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    LOG_COLORS = {\n",
    "        'DEBUG': 'cyan',\n",
    "        'INFO': 'white',\n",
    "        'WARNING': 'green',\n",
    "        'ERROR': 'purple',\n",
    "        'CRITICAL': 'red',\n",
    "    }\n",
    "    KAGGLE_DATASETS = [\n",
    "        \"shuyangli94/food-com-recipes-and-user-interactions\",\n",
    "        # Add more datasets here as needed\n",
    "    ]\n",
    "    RETRY_ATTEMPTS = 2\n",
    "    LOG_FORMAT = '%(message)s'\n",
    "    LOG_DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    RETRY_EXCEPTIONS = (requests.exceptions.RequestException, OSError)\n",
    "\n",
    "    @classmethod\n",
    "    def dataset_destination_path(cls, dataset_name: str) -> Path:\n",
    "        return cls.DATA_DIRECTORY / dataset_name.split('/')[1]\n",
    "\n",
    "\n",
    "def setup_logging():\n",
    "    logger = colorlog.getLogger()\n",
    "    logger.setLevel(getattr(logging, Config.LOG_LEVEL))\n",
    "    console_handler = colorlog.StreamHandler()\n",
    "    console_handler.setFormatter(colorlog.ColoredFormatter(\n",
    "        Config.LOG_FORMAT_CONSOLE,\n",
    "        datefmt=Config.DATE_FORMAT,\n",
    "        log_colors=Config.LOG_COLORS))\n",
    "    file_handler = logging.FileHandler(Config.LOG_FILE)\n",
    "    file_handler.setFormatter(logging.Formatter(\n",
    "        Config.LOG_FORMAT_FILE,\n",
    "        datefmt=Config.DATE_FORMAT))\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "def execution_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time() - start_time\n",
    "        logging.info(f\"{func.__name__} executed in {end_time:.2f} seconds.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def retry_on_failure(max_tries=Config.RETRY_ATTEMPTS, exceptions=Config.RETRY_EXCEPTIONS):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        @backoff.on_exception(backoff.expo, exceptions, max_tries=max_tries,\n",
    "                              on_backoff=lambda details: logging.warning(\n",
    "                                  f\"Retry {details['tries']}/{max_tries} for {func.__name__} due to error, waiting {details.get('wait', 0):0.1f} seconds.\"),\n",
    "                              on_giveup=lambda details: logging.error(\n",
    "                                  f\"Giving up {func.__name__} after {details['tries']} tries\"))\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def kaggle_api_authenticate() -> KaggleApi:\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    return api\n",
    "\n",
    "\n",
    "@execution_time\n",
    "def download_and_extract_kaggle_dataset(dataset_name: str) -> None:\n",
    "    api = kaggle_api_authenticate()  # Move Kaggle API authentication inside the function\n",
    "    destination_path = Config.dataset_destination_path(dataset_name)\n",
    "    if not destination_path.exists() or not any(destination_path.iterdir()):\n",
    "        api.dataset_download_files(dataset_name, path=destination_path, unzip=True)\n",
    "        logging.info(f\"Dataset '{dataset_name}' extracted to {destination_path}.\")\n",
    "    else:\n",
    "        logging.info(f\"Dataset '{dataset_name}' already present, skipping download.\")\n",
    "\n",
    "\n",
    "@execution_time\n",
    "def download_and_extract_kaggle_datasets(datasets: List[str]) -> None:\n",
    "    Parallel(n_jobs=Config.N_JOBS)(delayed(download_and_extract_kaggle_dataset)(dataset_name) for dataset_name in datasets)\n",
    "\n",
    "\n",
    "@execution_time\n",
    "def find_csv_files(datasets: List[str]) -> List[str]:\n",
    "    all_csv_files = sum(Parallel(n_jobs=Config.N_JOBS)(delayed(find_csv_file)(dataset_name) for dataset_name in datasets), [])\n",
    "    logging.info(f\"Found {len(all_csv_files)} total CSV files in the datasets.\")\n",
    "    return all_csv_files\n",
    "\n",
    "\n",
    "def find_csv_file(dataset_name: str) -> List[str]:\n",
    "    csv_files = [str(file) for file in Config.dataset_destination_path(dataset_name).rglob('*.csv')]\n",
    "    return csv_files\n",
    "\n",
    "\n",
    "@execution_time\n",
    "def main():\n",
    "    setup_logging()\n",
    "    logging.warning(\"Data setup initiated...\")\n",
    "    download_and_extract_kaggle_datasets(Config.KAGGLE_DATASETS)\n",
    "    csv_files = find_csv_files(Config.KAGGLE_DATASETS)\n",
    "    \n",
    "    if csv_files:\n",
    "        # Extract the folder name from the first file path\n",
    "        dataset_folder = Path(csv_files[0]).parent.relative_to(Config.DATA_DIRECTORY).as_posix()\n",
    "        # Extract just the file names\n",
    "        csv_file_names = [Path(file).name for file in csv_files]\n",
    "        \n",
    "        # Inserting a newline between the dataset folder path and the CSV files list\n",
    "        logging.info(f\"Dataset extracted to ../data/{dataset_folder}\\nCSV files: {', '.join(csv_file_names)}\")\n",
    "    else:\n",
    "        logging.info(\"No CSV files found.\")\n",
    "    \n",
    "    logging.warning(\"Dataset setup completed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
